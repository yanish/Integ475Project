head(diamonds)
library(gcookbook) # has data we want!
head(diamonds)
library(ggplot2)
library(dplyr)
library(gcookbook) # has data we want!
# let's use some data that comes with ggplot
head(diamonds)
devtools::install_github("rstudio/rmarkdown")
install.packages("rmarkdown")
install.packages("tm")
install.packages("lda")
install.packages("LDAvis")
install.packages("servr")
devtools::install_github("cpsievert/LDAvisData")
library(tm)
library(lda)
library(LDAvis)
library(servr)
library(LDAvisData)
install.packages("devtools")
library(tm)
library(lda)
library(LDAvis)
library(servr)
library(LDAvisData)
install.packages("devtools")
devtools::install_github("cpsievert/LDAvisData")
library(LDAvisData)
data(reviews, package = "LDAvisData")
## TOPIC MODELING IN R
# this is based on a tutorial by: http://cpsievert.github.io/LDAvis/reviews/reviews.html
# this is (mostly) code from their tutorial, but you will want to run it while reading their post
# because it provides an explanation of what the code is doing.
install.packages("tm")
install.packages("lda")
install.packages("LDAvis")
install.packages("servr")
install.packages("devtools")
devtools::install_github("cpsievert/LDAvisData")
library(tm)
library(lda)
library(LDAvis)
library(servr)
library(LDAvisData)
# get Cornell movie review data
data(reviews, package = "LDAvisData")
# let's get some stop words
stop_words <- stopwords("SMART")
# pre-processing:
# note: gsub is pattern matching and replacement in R
reviews <- gsub("'", "", reviews)  # remove apostrophes
reviews <- gsub("[[:punct:]]", " ", reviews)  # replace punctuation with space
reviews <- gsub("[[:cntrl:]]", " ", reviews)  # replace control characters with space
reviews <- gsub("^[[:space:]]+", "", reviews) # remove whitespace at beginning of documents
reviews <- gsub("[[:space:]]+$", "", reviews) # remove whitespace at end of documents
reviews <- tolower(reviews)  # force to lowercase
# tokenize on space and output as a list:
doc.list <- strsplit(reviews, "[[:space:]]+")
# compute the table of terms:
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)
# remove terms that are stop words or occur fewer than 5 times:
del <- names(term.table) %in% stop_words | term.table < 5
term.table <- term.table[!del]
vocab <- names(term.table)
# now put the documents into the format required by the lda package:
get.terms <- function(x) {
index <- match(x, vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
install.packages("tm")
install.packages("LDAvis")
install.packages("servr")
install.packages("devtools")
install.packages("lda")
## TOPIC MODELING IN R
# this is based on a tutorial by: http://cpsievert.github.io/LDAvis/reviews/reviews.html
# this is (mostly) code from their tutorial, but you will want to run it while reading their post
# because it provides an explanation of what the code is doing.
install.packages("tm")
install.packages("lda")
install.packages("LDAvis")
install.packages("servr")
install.packages("devtools")
devtools::install_github("cpsievert/LDAvisData")
library(tm)
library(lda)
library(LDAvis)
library(servr)
library(LDAvisData)
# get Cornell movie review data
data(reviews, package = "LDAvisData")
# let's get some stop words
stop_words <- stopwords("SMART")
# pre-processing:
# note: gsub is pattern matching and replacement in R
reviews <- gsub("'", "", reviews)  # remove apostrophes
reviews <- gsub("[[:punct:]]", " ", reviews)  # replace punctuation with space
reviews <- gsub("[[:cntrl:]]", " ", reviews)  # replace control characters with space
reviews <- gsub("^[[:space:]]+", "", reviews) # remove whitespace at beginning of documents
reviews <- gsub("[[:space:]]+$", "", reviews) # remove whitespace at end of documents
reviews <- tolower(reviews)  # force to lowercase
# tokenize on space and output as a list:
doc.list <- strsplit(reviews, "[[:space:]]+")
# compute the table of terms:
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)
# remove terms that are stop words or occur fewer than 5 times:
del <- names(term.table) %in% stop_words | term.table < 5
term.table <- term.table[!del]
vocab <- names(term.table)
# now put the documents into the format required by the lda package:
get.terms <- function(x) {
index <- match(x, vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
# NOTE: if you are interested, here is a nice blog post on lapply and related methods: http://www.r-bloggers.com/using-apply-sapply-lapply-in-r/
# MODEL FITTING
# Compute some statistics related to the data set:
D <- length(documents)  # number of documents (2,000)
W <- length(vocab)  # number of terms in the vocab (14,568)
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # total number of tokens in the data (546,827)
term.frequency <- as.integer(term.table)  # frequencies of terms in the corpus [8939, 5544, 2411, 2410, 2143, ...]
# MCMC and model tuning parameters:
K <- 20
G <- 5000
alpha <- 0.02
eta <- 0.02
# Fit the model:
set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
t2 <- Sys.time()
t2 - t1  # about 24 minutes on laptop
theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
MovieReviews <- list(phi = phi,
theta = theta,
doc.length = doc.length,
vocab = vocab,
term.frequency = term.frequency)
# visualize!
json <- createJSON(phi = MovieReviews$phi,
theta = MovieReviews$theta,
doc.length = MovieReviews$doc.length,
vocab = MovieReviews$vocab,
term.frequency = MovieReviews$term.frequency)
serVis(json, out.dir = 'vis', open.browser = FALSE)
library(igraph)
library(dplyr)
library(networkD3)
setwd("Users/Yanish/Documents/Fall_2015/Integ_475/mini_project_2/Integ475Project")
setwd("Users/Yanish/Documents/Fall_2015/Integ_475/mini_project_2/Integ475Project")
setwd("/Users/Yanish/Documents/Fall_2015/Integ_475/mini_project_2/Integ475Project")
WN <- read.graph("networks/net.graphml", format = "graphml" )
network <- read.graph("networks/net.graphml", format = "graphml" )
plot (network)
